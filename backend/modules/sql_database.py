# modules/sql_database.py
import os
import logging
import sqlalchemy
from sqlalchemy import (create_engine, Column, Integer, String, Text, DateTime,
                        ForeignKey, Boolean, Table, or_, func, desc, Date)
from sqlalchemy.orm import sessionmaker, relationship, scoped_session, joinedload
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy import Enum as SQLAlchemyEnum # For status
import enum
from datetime import datetime
from contextlib import contextmanager
from typing import Optional, List, Dict, Any
from dateutil.parser import parse as parse_datetime # Use dateutil for robust parsing
import asyncio

# Configure logging (ideally configured at application entry point)
logger = logging.getLogger(__name__)

# Base class for SQLAlchemy models
Base = declarative_base()

# --- Model Definitions ---

# Association table for the many-to-many relationship between Papers and Authors
paper_authors = Table(
    'paper_authors', Base.metadata,
    Column('paper_id', Integer, ForeignKey('papers.id', ondelete='CASCADE'), primary_key=True),
    Column('author_id', Integer, ForeignKey('authors.id', ondelete='CASCADE'), primary_key=True)
)

class Author(Base):
    """Represents an author of a paper."""
    __tablename__ = 'authors'
    id = Column(Integer, primary_key=True)
    # Ensure name is indexed for faster lookups
    name = Column(String(255), nullable=False, unique=True, index=True)
    # Relationship to papers (many-to-many)
    papers = relationship(
        "Paper", secondary=paper_authors,
        back_populates="authors", cascade="all, delete" # Cascade deletes on Author deletion
    )

    def __repr__(self):
        return f"<Author(id={self.id}, name='{self.name}')>"

class Journal(Base):
    """Represents a journal where a paper was published."""
    __tablename__ = 'journals'
    id = Column(Integer, primary_key=True)
    # Ensure name is indexed
    name = Column(String(255), nullable=False, unique=True, index=True)
    # Relationship to papers (one-to-many)
    papers = relationship("Paper", back_populates="journal") # Don't cascade delete papers if journal deleted

    def __repr__(self):
        return f"<Journal(id={self.id}, name='{self.name}')>"

class Source(Base):
    """Represents the source database (e.g., PubMed, Google Scholar)."""
    __tablename__ = 'sources'
    id = Column(Integer, primary_key=True)
    name = Column(String(50), nullable=False, unique=True, index=True)

    # Relationship to papers (one-to-many)
    papers = relationship("Paper", back_populates="source") # Don't cascade delete papers if source deleted

    def __repr__(self):
        return f"<Source(id={self.id}, name='{self.name}')>"

class Paper(Base):
    """Represents a scientific paper."""
    __tablename__ = 'papers'
    id = Column(Integer, primary_key=True)
    title = Column(String(500), nullable=False, index=True) # Index title for searching

    # Store publication date as Date, not DateTime, unless time is relevant
    publication_date = Column(Date, index=True) # Changed to Date, added index
    url = Column(String(500), unique=True, nullable=False, index=True) # Index URL for fast lookups
    
    # Store the actual abstract text retrieved from the source
    abstract = Column(Text)
    
    # Store the summary generated by the AI model (populated later)
    summary = Column(Text, nullable=True) # Allow null initially
    
    # Store vector IDs (assuming they are strings, adjust if needed)
    abstract_vector_id = Column(String(255), index=True, nullable=True) # Allow null
    summary_vector_id = Column(String(255), index=True, nullable=True) # Allow null

    # Foreign Keys to related tables
    # Use ondelete='SET NULL' so deleting a journal/source doesn't delete the paper
    journal_id = Column(Integer, ForeignKey('journals.id', ondelete='SET NULL'), nullable=True)
    source_id = Column(Integer, ForeignKey('sources.id', ondelete='SET NULL'), nullable=True)

    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)

    # Relationships
    journal = relationship("Journal", back_populates="papers")
    authors = relationship("Author", secondary=paper_authors, back_populates="papers", lazy='joined') # Use joined loading for authors
    source = relationship("Source", back_populates="papers")

    def __repr__(self):
        journal_name = self.journal.name if self.journal else "N/A"
        pub_date_str = self.publication_date.isoformat() if self.publication_date else "N/A"
        return f"<Paper(id={self.id}, title='{self.title[:30]}...', journal='{journal_name}', date='{pub_date_str}')>"

    def to_dict(self) -> Dict[str, Any]:
        """Convert Paper object to dictionary, including related objects."""
        return {
            "id": self.id,
            "title": self.title,
            "abstract": self.abstract,
            "summary": self.summary,
            "url": self.url,
            "publication_date": self.publication_date.isoformat() if self.publication_date else None,
            "source": self.source.name if self.source else "Unknown",
            "journal": self.journal.name if self.journal else "Unknown",
            "authors": [author.name for author in self.authors] if self.authors else [],
            "abstract_vector_id": self.abstract_vector_id,
            "summary_vector_id": self.summary_vector_id,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat()
        }

# Enum for query status
class QueryStatus(enum.Enum):
    PROCESSING = "processing"
    COMPLETED = "completed"
    ERROR = "error"

class Query(Base):
    """Represents a user query cached in the database."""
    __tablename__ = 'queries'
    id = Column(Integer, primary_key=True)

    # Store the original user query text
    query_text = Column(String(500), nullable=False, index=True)
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)

    # Store when the cache entry expires
    expires_at = Column(DateTime, nullable=True)  # Allow null if processing never finishes? Or set for future?

    # Store status and error message columns to the Query model
    status = Column(SQLAlchemyEnum(QueryStatus), default=QueryStatus.PROCESSING, nullable=False, index=True)
    error_message = Column(Text, nullable=True) # Store error details if status is ERROR
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)

    # Relationship to the papers found for this query
    results = relationship("QueryResult", back_populates="query", cascade="all, delete-orphan")

    def __repr__(self):
        return f"<Query(id={self.id}, text='{self.query_text[:30]}...', status='{self.status.value}')>"

class QueryResult(Base):
    """Associates a Paper with a Query, storing its rank in the results."""
    __tablename__ = 'query_results'
    id = Column(Integer, primary_key=True) # Use own primary key
    query_id = Column(Integer, ForeignKey('queries.id', ondelete='CASCADE'), nullable=False)
    paper_id = Column(Integer, ForeignKey('papers.id', ondelete='CASCADE'), nullable=False)
    # Rank of the paper within the query results
    rank = Column(Integer, nullable=False)

    # Relationships
    query = relationship("Query", back_populates="results")
    # Eagerly load paper details when fetching QueryResult
    paper = relationship("Paper", lazy='joined')

    # Add unique constraint for query_id and paper_id? Or query_id and rank?
    # __table_args__ = (sqlalchemy.UniqueConstraint('query_id', 'paper_id', name='uq_query_paper'),)
    __table_args__ = (sqlalchemy.UniqueConstraint('query_id', 'rank', name='uq_query_rank'),)


    def __repr__(self):
        paper_title = self.paper.title[:20] + "..." if self.paper else "N/A"
        return f"<QueryResult(query_id={self.query_id}, paper='{paper_title}', rank={self.rank})>"

# --- Database Interaction Class ---

class SQLDatabase:
    """Handles interactions with the SQL database using SQLAlchemy."""

    def __init__(self, connection_string: Optional[str] = None):
        """
        Initializes the database connection and creates tables if they don't exist.

        Args:
            connection_string: SQLAlchemy connection string. Defaults to SQLite
                               in './data/papers.db' or path from SQL_DB_PATH env var.
        """
        if connection_string is None:
            # Default to a file in a 'data' subdirectory relative to this script's location
            # Or use environment variable if set
            default_db_dir = os.path.join(os.path.dirname(__file__), '..', 'data') # Assumes modules/ is one level down
            db_path_env = os.environ.get("SQL_DB_PATH")
            if db_path_env:
                 db_path = db_path_env
                 os.makedirs(os.path.dirname(db_path), exist_ok=True)
            else:
                os.makedirs(default_db_dir, exist_ok=True)
                db_path = os.path.join(default_db_dir, "papers.db")

            connection_string = f"sqlite:///{os.path.abspath(db_path)}"
            logger.info(f"No connection string provided, using default SQLite DB: {connection_string}")

        try:
            self.engine = create_engine(
                connection_string,
                # Enable echo for debugging SQL statements if needed
                # echo=True,
                # For SQLite, disable same-thread check for async compatibility (though calls are sync)
                connect_args={"check_same_thread": False} if connection_string.startswith("sqlite") else {},
                # Pool settings can be adjusted for performance, esp. for non-SQLite DBs
                # pool_size=10, max_overflow=20
            )
            # Use scoped_session for thread-local session management, crucial for web apps
            self.Session = scoped_session(sessionmaker(bind=self.engine, expire_on_commit=False)) # expire_on_commit=False is often safer

            # Create all tables defined in Base's metadata
            Base.metadata.create_all(self.engine)
            logger.info(f"Successfully connected to DB and ensured tables exist: {connection_string}")

        except SQLAlchemyError as e:
            logger.exception(f"Database connection failed for {connection_string}: {e}")
            raise # Re-raise the exception to halt application startup if DB is essential


    @contextmanager
    def _session_scope(self):
        """Provide a transactional scope around a series of operations."""
        session = self.Session()
        logger.debug(f"DB Session {id(session)} obtained.")
        try:
            yield session
            session.commit()
            logger.debug(f"DB Session {id(session)} committed.")
        except SQLAlchemyError as e:
            logger.error(f"DB Session {id(session)} rollback due to error: {e}", exc_info=True)
            session.rollback()
            raise # Re-raise the original SQLAlchemy exception
        except Exception as e:
            logger.error(f"DB Session {id(session)} rollback due to unexpected error: {e}", exc_info=True)
            session.rollback()
            raise # Re-raise other exceptions
        finally:
            logger.debug(f"DB Session {id(session)} closed.")
            self.Session.remove() # Explicitly remove the session from the registry


    def _parse_date(self, date_string: Optional[str]) -> Optional[Date]:
        """Safely parse various date string formats into a date object."""
        if not date_string or not isinstance(date_string, str):
            return None
        try:
            # Use dateutil.parser for flexibility, return only the date part
            return parse_datetime(date_string).date()
        except (ValueError, TypeError) as e:
            logger.warning(f"Could not parse date string '{date_string}': {e}")
            # Attempt simpler formats if complex parsing fails
            for fmt in ("%Y", "%Y %b", "%Y %B"): # Year only, Year Month formats
                 try:
                     return datetime.strptime(date_string, fmt).date().replace(day=1, month=1 if fmt == "%Y" else None) # Default day/month
                 except ValueError:
                     continue
            return None # Return None if all parsing attempts fail


    async def add_paper(self,
                        paper_data: Dict[str, Any],
                        abstract_vector_id: Optional[str] = None) -> Optional[int]:
        """
        Adds a new paper to the database or updates minimal info if it already exists by URL.
        This is intended for the initial insertion after retrieval from PubMed/Scholar.
        The summary and summary_vector_id should be added later using `update_paper_summary`.

        Args:
            paper_data: Dictionary containing paper details (title, authors, url, abstract, etc.)
                        Expected keys match PaperInfo.to_dict().
            abstract_vector_id: Optional vector ID for the paper's abstract.

        Returns:
            The integer ID of the added or existing paper, or None if insertion failed.
        """
        required_fields = ["url", "title", "abstract", "source"]
        if not all(paper_data.get(field) for field in required_fields):
            logger.error(f"Cannot add paper, missing required fields ({required_fields}) in paper_data: {paper_data.get('url', 'URL missing')}")
            return None

        paper_url = paper_data["url"]
        logger.info(f"Attempting to add/update paper with URL: {paper_url}")

        try:
            with self._session_scope() as session:
                # --- Check if paper already exists by URL ---
                paper = session.query(Paper).filter(Paper.url == paper_url).first()

                if paper:
                    # --- Paper exists: Update timestamp and abstract vector ID if provided ---
                    logger.info(f"Paper with URL {paper_url} already exists (ID: {paper.id}). Updating timestamp.")
                    paper.updated_at = datetime.utcnow()
                    # Update abstract_vector_id if provided and different
                    if abstract_vector_id is not None and paper.abstract_vector_id != abstract_vector_id:
                        logger.info(f"Updating abstract_vector_id for paper {paper.id} to {abstract_vector_id}")
                        paper.abstract_vector_id = abstract_vector_id
                    session.flush() # Ensure ID is available before returning
                    return paper.id
                else:
                    # --- Paper doesn't exist: Create new paper ---
                    logger.info(f"Paper with URL {paper_url} not found. Creating new entry.")

                    # Helper to get or create related entities (Author, Journal, Source)
                    def get_or_create(model, **kwargs):
                        # Attempt to find existing instance
                        instance = session.query(model).filter_by(**kwargs).first()
                        if instance:
                            return instance
                        else:
                            # Create new instance if not found
                            logger.debug(f"Creating new {model.__name__} with args: {kwargs}")
                            instance = model(**kwargs)
                            session.add(instance)
                            # Flush here to get potential IDs if needed, but commit happens at end of scope
                            session.flush()
                            return instance

                    # Get or create Source and Journal
                    source_name = paper_data.get("source", "Unknown Source")
                    journal_name = paper_data.get("journal", "Unknown Journal")
                    source = get_or_create(Source, name=source_name)
                    journal = get_or_create(Journal, name=journal_name)

                    # Parse publication date
                    pub_date = self._parse_date(paper_data.get("publication_date"))

                    # Create the new Paper object
                    new_paper = Paper(
                        title=paper_data.get("title"),
                        publication_date=pub_date,
                        url=paper_url,
                        abstract=paper_data.get("abstract"),
                        abstract_vector_id=abstract_vector_id,
                        # Summary and summary_vector_id are initially null
                        summary=None,
                        summary_vector_id=None,
                        journal=journal,
                        source=source,
                        # Timestamps are set by default values
                    )
                    session.add(new_paper)
                    logger.info(f"Added new Paper object for URL {paper_url} to session.")

                    # Process authors
                    author_names = paper_data.get("authors", [])
                    if not author_names:
                         logger.warning(f"No authors found for paper URL {paper_url}.")

                    for name in author_names:
                         if name and isinstance(name, str) and name.strip(): # Basic validation
                            author = get_or_create(Author, name=name.strip())
                            # Add relationship (SQLAlchemy handles the association table)
                            new_paper.authors.append(author)
                         else:
                             logger.warning(f"Skipping invalid author name: '{name}' for paper {paper_url}")


                    # Flush the session to assign an ID to the new paper before returning it
                    session.flush()
                    logger.info(f"Successfully created new paper with ID: {new_paper.id} for URL: {paper_url}")
                    return new_paper.id

        except SQLAlchemyError as e:
            logger.exception(f"Database error occurred while adding/updating paper {paper_url}: {e}")
            return None # Indicate failure
        except Exception as e:
             logger.exception(f"Unexpected error occurred while adding/updating paper {paper_url}: {e}")
             return None # Indicate failure


    async def update_paper_summary(self,
                                   paper_id: int,
                                   summary: str,
                                   summary_vector_id: Optional[str] = None) -> bool:
        """
        Updates the summary and summary_vector_id for an existing paper.
        This should be called after the AI model generates the summary.

        Args:
            paper_id: The ID of the paper to update.
            summary: The generated summary text.
            summary_vector_id: Optional vector ID for the summary.

        Returns:
            True if the update was successful, False otherwise.
        """
        if not paper_id or not summary:
             logger.error(f"Invalid input for update_paper_summary: paper_id={paper_id}, summary provided={bool(summary)}")
             return False

        logger.info(f"Attempting to update summary for paper ID: {paper_id}")
        try:
            with self._session_scope() as session:
                # Find the paper by ID
                paper = session.query(Paper).filter(Paper.id == paper_id).first()
                if not paper:
                    logger.error(f"Cannot update summary: Paper with ID {paper_id} not found.")
                    return False
                # Update the fields
                paper.summary = summary
                paper.summary_vector_id = summary_vector_id
                paper.updated_at = datetime.utcnow() # Ensure updated_at reflects this change
                logger.info(f"Successfully updated summary and vector ID for paper ID: {paper_id}")
                return True
        except SQLAlchemyError as e:
            logger.exception(f"Database error occurred while updating summary for paper ID {paper_id}: {e}")
            return False
        except Exception as e:
             logger.exception(f"Unexpected error occurred while updating summary for paper ID {paper_id}: {e}")
             return False


    async def get_recent_papers(self, limit: int = 10, source_name: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get recently added papers, optionally filtered by source name."""
        logger.info(f"Fetching {limit} recent papers" + (f" from source '{source_name}'" if source_name else ""))
        try:
            with self._session_scope() as session:
                query = session.query(Paper)\
                    .options(
                        joinedload(Paper.authors),
                        joinedload(Paper.journal),
                        joinedload(Paper.source)
                    )\
                    .order_by(desc(Paper.created_at)) # Order by creation time

                if source_name:
                    # Filter by joining with Source table
                    query = query.join(Source).filter(Source.name == source_name)

                papers = query.limit(limit).all()
                results = [paper.to_dict() for paper in papers]
                logger.info(f"Found {len(results)} recent papers.")
                return results
        except SQLAlchemyError as e:
            logger.exception(f"Database error getting recent papers: {e}")
            return []
        except Exception as e:
             logger.exception(f"Unexpected error getting recent papers: {e}")
             return []


    async def get_papers_by_ids(self, paper_ids: List[int]) -> List[Dict[str, Any]]:
        """
        Retrieves full details for multiple papers based on their primary IDs.
        Uses joined loading for efficiency.

        Args:
            paper_ids: A list of integer paper IDs.

        Returns:
            A list of dictionaries, where each dictionary represents a paper.
            Returns an empty list if no IDs are provided or no papers are found.
        """
        if not paper_ids:
            return []

        logger.info(f"Fetching papers for IDs: {paper_ids}")
        try:
            with self._session_scope() as session:
                # Query papers, using joinedload to eagerly fetch related data in one go
                papers = session.query(Paper)\
                    .options(
                        joinedload(Paper.authors),
                        joinedload(Paper.journal),
                        joinedload(Paper.source)
                    )\
                    .filter(Paper.id.in_(paper_ids))\
                    .all()

                if not papers:
                    logger.warning(f"No papers found for IDs: {paper_ids}")
                    return []

                # Convert results to dictionaries
                results = [paper.to_dict() for paper in papers]
                logger.info(f"Successfully fetched {len(results)} papers for IDs: {paper_ids}")
                return results

        except SQLAlchemyError as e:
            logger.exception(f"Database error occurred while fetching papers by IDs {paper_ids}: {e}")
            return []
        except Exception as e:
             logger.exception(f"Unexpected error occurred while fetching papers by IDs {paper_ids}: {e}")
             return []


    async def get_cached_query(self, query_text: str) -> Optional[Dict[str, Any]]:
        """
        Get cached status, error message, and paper results for a given query text.
        Returns None if the query doesn't exist in the cache.
        """
        logger.info(f"Checking cache for query: '{query_text[:50]}...'")
        try:
            with self._session_scope() as session:
                now = datetime.utcnow()
                # Find the query
                query = session.query(Query)\
                    .options(
                        # Eagerly load results and their associated papers with details only if completed
                        joinedload(Query.results).joinedload(QueryResult.paper).joinedload(Paper.authors),
                        joinedload(Query.results).joinedload(QueryResult.paper).joinedload(Paper.journal),
                        joinedload(Query.results).joinedload(QueryResult.paper).joinedload(Paper.source),
                     )\
                    .filter(Query.query_text == query_text)\
                    .first() # Get the query regardless of expiry for status check

                if not query:
                    logger.info(f"Cache miss: Query '{query_text[:50]}...' not found.")
                    return None # Query never submitted or somehow deleted

                # --- Prepare response based on status ---
                response_data = {
                    "query_text": query.query_text,
                    "status": query.status,
                    "error_message": query.error_message,
                    "papers": [] # Default to empty list
                }

                # Check expiry only if completed successfully
                if query.status == QueryStatus.COMPLETED and query.expires_at <= now:
                     logger.info(f"Cache hit but expired for completed query: '{query_text[:50]}...'")
                     # Optionally: Update status back to processing or delete? Or just return expired status?
                     # Let's return the expired data but maybe frontend handles this?
                     # Or return status 'expired'? For now, return completed but empty papers.
                     response_data["status"] = QueryStatus.PROCESSING # Treat as expired, trigger re-run
                     return response_data


                if query.status == QueryStatus.COMPLETED:
                    logger.info(f"Cache hit for completed query: '{query_text[:50]}...' (Query ID: {query.id})")
                    # Extract paper details only if completed
                    results = []
                    sorted_query_results = sorted(query.results, key=lambda qr: qr.rank)
                    for qr in sorted_query_results:
                        if qr.paper:
                            results.append(qr.paper.to_dict())
                        else:
                            logger.warning(f"Cached query result (ID: {qr.id}) points to a missing paper (Paper ID: {qr.paper_id}). Skipping.")
                    response_data["papers"] = results
                    logger.info(f"Returning {len(results)} cached papers for completed query.")
                elif query.status == QueryStatus.ERROR:
                     logger.info(f"Cache hit for errored query: '{query_text[:50]}...' (Query ID: {query.id})")
                     # Error message is already in response_data
                else: # Status is PROCESSING
                     logger.info(f"Cache hit for processing query: '{query_text[:50]}...' (Query ID: {query.id})")
                     # No papers or error message needed

                return response_data

        except SQLAlchemyError as e:
            logger.exception(f"Database error checking cache for query '{query_text[:50]}...': {e}")
            # Indicate error in retrieval itself
            return {"query_text": query_text, "status": QueryStatus.ERROR, "error_message": "Database error during cache check", "papers": []}
        except Exception as e:
             logger.exception(f"Unexpected error checking cache for query '{query_text[:50]}...': {e}")
             return {"query_text": query_text, "status": QueryStatus.ERROR, "error_message": "Unexpected error during cache check", "papers": []}


    async def cache_query(self,
                          query_text: str,
                          paper_ids: Optional[List[int]] = None,
                          status: QueryStatus = QueryStatus.PROCESSING, # Default to processing
                          error_message: Optional[str] = None,
                          expires_days: int = 7) -> Optional[int]:
        """
        Creates or updates a cache entry for a query text, storing status,
        error message (if any), and associated paper IDs (if completed).
        """
        if not query_text:
             logger.warning("Attempted to cache query with empty text.")
             return None

        log_status = f"status={status.value}" + (f", error='{error_message[:50]}...'" if error_message else "")
        logger.info(f"Caching query: '{query_text[:50]}...' ({log_status}, papers: {len(paper_ids) if paper_ids else 'N/A'})")

        try:
            with self._session_scope() as session:
                # --- Find existing query or create new one ---
                query = session.query(Query).filter(Query.query_text == query_text).first()
                now = datetime.utcnow()
                expires_at = now + datetime.timedelta(days=expires_days) if status == QueryStatus.COMPLETED else None # Only set expiry on completion

                if query:
                    logger.debug(f"Updating existing Query cache entry (ID: {query.id})")
                    query.status = status
                    query.error_message = error_message
                    query.updated_at = now
                    query.expires_at = expires_at # Update expiry based on new status

                    # If updating to COMPLETED, clear old results and add new ones
                    if status == QueryStatus.COMPLETED:
                         # Delete existing QueryResult links for this query first
                         session.query(QueryResult).filter(QueryResult.query_id == query.id).delete(synchronize_session=False)
                         # Add new QueryResult entries
                         if paper_ids:
                              query_results = []
                              for rank, paper_id in enumerate(paper_ids):
                                  result = QueryResult(query_id=query.id, paper_id=paper_id, rank=rank)
                                  query_results.append(result)
                              if query_results:
                                  session.add_all(query_results)
                                  logger.info(f"Updated cached links for {len(query_results)} papers for query ID: {query.id}")
                         else:
                              logger.info(f"Query ID {query.id} completed with no paper results.")
                    session.flush()
                    return query.id

                else:
                     # --- Create new query cache entry ---
                     logger.debug(f"Creating new Query cache entry for '{query_text[:50]}...'")
                     new_query = Query(
                        query_text=query_text,
                        created_at=now,
                        updated_at=now,
                        expires_at=expires_at,
                        status=status,
                        error_message=error_message
                     )
                     session.add(new_query)
                     session.flush() # Get the new_query.id

                     # Add QueryResult links only if completed and paper IDs exist
                     if status == QueryStatus.COMPLETED and paper_ids:
                          query_results = []
                          for rank, paper_id in enumerate(paper_ids):
                              result = QueryResult(query_id=new_query.id, paper_id=paper_id, rank=rank)
                              query_results.append(result)
                          if query_results:
                              session.add_all(query_results)
                              logger.info(f"Cached {len(query_results)} paper links for new query ID: {new_query.id}")
                     elif status == QueryStatus.COMPLETED:
                           logger.info(f"New query ID {new_query.id} completed with no paper results.")

                     return new_query.id
                
        except SQLAlchemyError as e:
            logger.exception(f"Database error caching query '{query_text[:50]}...': {e}")
            return None
        except Exception as e:
             logger.exception(f"Unexpected error caching query '{query_text[:50]}...': {e}")
             return None


    async def get_recent_queries(self, limit: int = 20) -> List[Dict[str, Any]]:
        """Retrieves the most recent queries based on creation time."""
        logger.info(f"Fetching {limit} recent queries from history.")
        try:
            with self._session_scope() as session:
                queries = session.query(Query.id, Query.query_text, Query.created_at)\
                    .order_by(desc(Query.created_at))\
                    .limit(limit)\
                    .all()
                # Convert Row objects to dictionaries
                results = [{"id": q.id, "query_text": q.query_text, "created_at": q.created_at.isoformat()} for q in queries]
                logger.info(f"Found {len(results)} recent queries.")
                return results
        except SQLAlchemyError as e:
            logger.exception(f"Database error getting recent queries: {e}")
            return []
        except Exception as e:
             logger.exception(f"Unexpected error getting recent queries: {e}")
             return []
        

    async def get_paper_by_url(self, url: str) -> Optional[Dict[str, Any]]:
        """Get a single paper's details by its unique URL."""
        if not url: return None
        logger.info(f"Fetching paper by URL: {url}")
        try:
            with self._session_scope() as session:
                paper = session.query(Paper)\
                    .options(
                        joinedload(Paper.authors),
                        joinedload(Paper.journal),
                        joinedload(Paper.source)
                    )\
                    .filter(Paper.url == url)\
                    .first()

                if paper:
                    logger.info(f"Found paper ID {paper.id} for URL: {url}")
                    return paper.to_dict()
                else:
                    logger.info(f"Paper not found for URL: {url}")
                    return None
        except SQLAlchemyError as e:
            logger.exception(f"Database error getting paper by URL {url}: {e}")
            return None
        except Exception as e:
             logger.exception(f"Unexpected error getting paper by URL {url}: {e}")
             return None

    async def get_papers_by_vector_ids(self, vector_ids: List[str]) -> List[Dict[str, Any]]:
        """
        Get papers matching a list of abstract_vector_id or summary_vector_id.
        Useful for retrieving papers after a vector similarity search.
        """
        if not vector_ids: return []
        logger.info(f"Fetching papers by {len(vector_ids)} vector IDs.")
        try:
            with self._session_scope() as session:
                papers = session.query(Paper)\
                    .options(
                        joinedload(Paper.authors),
                        joinedload(Paper.journal),
                        joinedload(Paper.source)
                    )\
                    .filter(
                        or_(
                            Paper.abstract_vector_id.in_(vector_ids),
                            Paper.summary_vector_id.in_(vector_ids)
                        )
                    ).all()

                results = [paper.to_dict() for paper in papers]
                logger.info(f"Found {len(results)} papers matching vector IDs.")
                return results
        except SQLAlchemyError as e:
            logger.exception(f"Database error getting papers by vector IDs: {e}")
            return []
        except Exception as e:
             logger.exception(f"Unexpected error getting papers by vector IDs: {e}")
             return []

    async def search_papers(self, search_term: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Basic text search across paper titles, abstracts, and summaries using SQL LIKE.
        Note: For large datasets, full-text search capabilities (e.g., PostgreSQL FTS, SQLite FTS5)
              or vector search would be much more efficient and effective.
        """
        if not search_term: return []
        logger.info(f"Performing basic text search for '{search_term}' (limit {limit}).")
        # Basic sanitization of search term for LIKE
        search_pattern = f"%{search_term.strip('%_')}%"
        try:
            with self._session_scope() as session:
                papers = session.query(Paper)\
                    .options(
                        joinedload(Paper.authors),
                        joinedload(Paper.journal),
                        joinedload(Paper.source)
                    )\
                    .filter(
                        or_(
                            Paper.title.ilike(search_pattern), # Case-insensitive LIKE
                            Paper.abstract.ilike(search_pattern),
                            Paper.summary.ilike(search_pattern)
                        )
                    )\
                    .order_by(desc(Paper.publication_date), desc(Paper.created_at))\
                        .limit(limit)\
                            .all()

                results = [paper.to_dict() for paper in papers]
                logger.info(f"Found {len(results)} papers matching text search.")
                return results
        except SQLAlchemyError as e:
            logger.exception(f"Database error during text search for '{search_term}': {e}")
            return []
        except Exception as e:
             logger.exception(f"Unexpected error during text search for '{search_term}': {e}")
             return []


    async def get_statistics(self) -> Dict[str, Any]:
        """Get various statistics about the database content."""
        logger.info("Calculating database statistics.")
        try:
            with self._session_scope() as session:
                stats = {}
                stats["total_papers"] = session.query(func.count(Paper.id)).scalar()
                stats["total_authors"] = session.query(func.count(Author.id)).scalar()
                stats["total_journals"] = session.query(func.count(Journal.id)).scalar()
                stats["total_sources"] = session.query(func.count(Source.id)).scalar()
                stats["total_cached_queries"] = session.query(func.count(Query.id)).scalar()

                # Source distribution
                source_counts_query = session.query(Source.name, func.count(Paper.id))\
                    .outerjoin(Paper, Source.id == Paper.source_id)\
                    .group_by(Source.name)\
                    .all()
                stats["source_distribution"] = {name: count for name, count in source_counts_query}

                # Papers with/without summaries
                stats["papers_with_summary"] = session.query(func.count(Paper.id)).filter(Paper.summary != None).scalar()
                stats["papers_without_summary"] = stats["total_papers"] - stats["papers_with_summary"]

                # Date range of papers
                min_date, max_date = session.query(func.min(Paper.publication_date), func.max(Paper.publication_date)).one_or_none()
                stats["paper_date_range"] = {
                    "earliest": min_date.isoformat() if min_date else None,
                    "latest": max_date.isoformat() if max_date else None
                }

                stats["stats_calculated_at"] = datetime.utcnow().isoformat()
                logger.info(f"Database statistics calculated: {stats}")
                return stats

        except SQLAlchemyError as e:
            logger.exception(f"Database error calculating statistics: {e}")
            return {"error": "Failed to calculate statistics due to database error."}
        except Exception as e:
             logger.exception(f"Unexpected error calculating statistics: {e}")
             return {"error": "Failed to calculate statistics due to unexpected error."}


# --- Example Usage (Optional) ---
async def example_usage():
    print("--- SQLDatabase Example Usage ---")
    # Use in-memory SQLite for quick testing
    db = SQLDatabase(connection_string="sqlite:///:memory:")
    # Or use the default file-based DB
    # db = SQLDatabase()

    # 1. Add a paper (initial step)
    paper_info_1 = {
        "title": "Paper Title One", "authors": ["Author A", "Author B"],
        "publication_date": "2023-10-26", "journal": "Journal of Science",
        "abstract": "This is the abstract for paper one.", "url": "http://example.com/paper1",
        "source": "PubMed"
    }
    paper_id_1 = await db.add_paper(paper_info_1, abstract_vector_id="vec_abs_1")
    print(f"Added paper 1, ID: {paper_id_1}")

    paper_info_2 = {
        "title": "Second Paper Example", "authors": ["Author C"],
        "publication_date": "2024 Jan", "journal": "Nature Communications",
        "abstract": "Abstract for the second paper.", "url": "http://example.com/paper2",
        "source": "Google Scholar"
    }
    paper_id_2 = await db.add_paper(paper_info_2, abstract_vector_id="vec_abs_2")
    print(f"Added paper 2, ID: {paper_id_2}")

    # 2. Get papers by IDs (e.g., to get abstracts for summarization)
    papers_to_summarize = await db.get_papers_by_ids([paper_id_1, paper_id_2])
    print("\nPapers retrieved for summarization:")
    for p in papers_to_summarize:
        print(f"- ID: {p['id']}, Title: {p['title']}, Abstract: {p['abstract'][:30]}...")

    # 3. Update paper with generated summary (simulation)
    summary_1 = "This is the AI generated summary for paper one."
    summary_vec_1 = "vec_sum_1"
    update_success = await db.update_paper_summary(paper_id_1, summary_1, summary_vec_1)
    print(f"\nUpdated summary for paper {paper_id_1}: Success = {update_success}")

    summary_2 = "AI summary for the second paper is here."
    # update_success = await db.update_paper_summary(paper_id_2, summary_2) # Without vector ID
    # print(f"Updated summary for paper {paper_id_2}: Success = {update_success}")


    # 4. Retrieve paper again to see updated summary
    updated_paper_1 = await db.get_papers_by_ids([paper_id_1])
    if updated_paper_1:
        print("\nPaper 1 after summary update:")
        print(f"  Summary: {updated_paper_1[0].get('summary')}")
        print(f"  Summary Vec ID: {updated_paper_1[0].get('summary_vector_id')}")

    # 5. Cache a query
    query_text = "original user query about paper one"
    cached_query_id = await db.cache_query(query_text, [paper_id_1])
    print(f"\nCached query '{query_text[:20]}...' with ID: {cached_query_id}")

    # 6. Get cached query
    cached_results = await db.get_cached_query(query_text)
    print("\nRetrieved cached results:")
    for p in cached_results:
        print(f"- ID: {p['id']}, Title: {p['title']}")

    # 7. Get Stats
    stats = await db.get_statistics()
    print("\nDatabase Stats:")
    import json
    print(json.dumps(stats, indent=2))

    # 8. Test text search
    search_results = await db.search_papers("second paper")
    print("\nText search results for 'second paper':")
    for p in search_results:
        print(f"- ID: {p['id']}, Title: {p['title']}")

if __name__ == "__main__":
    # Run the example usage function
    asyncio.run(example_usage())
